{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kb10241024/PYNBs/blob/master/LeNet5_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmz9zZG1KrBy",
        "colab_type": "text"
      },
      "source": [
        "# LeNet-5 — LeCun et al\n",
        "**LeNet-5**, a 7 layer Convolutional Neural Network, was deployed in many banking systems to recognize hand-written numbers on cheques.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1026/1*avmwxTth2efzRlvSr6MaKg.jpeg)\n",
        "\n",
        "# LeNet-5 — Architecture [blog](https://medium.com/datadriveninvestor/five-powerful-cnn-architectures-b939c9ddd57b)\n",
        "The hand-written numbers were digitized into grayscale images of pixel size — 32×32. At that time, the computational capacity was limited and hence the technique wasn’t scalable to large scale images.\n",
        "Let’s understand the architecture of the model. The model contained 7 layers excluding the input layer. Since it is a relatively small architecture, let’s go layer by layer:\n",
        "# Layer 1:\n",
        "A convolutional layer with kernel size of 5×5, stride of 1×1 and 6 kernels in total. So the input image of size 32x32x1 gives an output of 28x28x6. Total params in layer = 5 * 5 * 6 + 6 (bias terms)\n",
        "# Layer 2: \n",
        "A pooling layer with 2×2 kernel size, stride of 2×2 and 6 kernels in total. This pooling layer acted a little differently than what we discussed in previous post. The input values in the receptive were summed up and then were multiplied to a trainable parameter (1 per filter), the result was finally added to a trainable bias (1 per filter). Finally, sigmoid activation was applied to the output. So, the input from previous layer of size 28x28x6 gets sub-sampled to 14x14x6. Total params in layer = [1 (trainable parameter) + 1 (trainable bias)] * 6 = 12\n",
        "# Layer 3: \n",
        "Similar to Layer 1, this layer is a convolutional layer with same configuration except it has 16 filters instead of 6. So the input from previous layer of size 14x14x6 gives an output of 10x10x16. Total params in layer = 5 * 5 * 16 + 16 = 416.\n",
        "# Layer 4: \n",
        "Again, similar to Layer 2, this layer is a pooling layer with 16 filters this time around. Remember, the outputs are passed through sigmoid activation function. The input of size 10x10x16 from previous layer gets sub-sampled to 5x5x16. Total params in layer = (1 + 1) * 16 = 32\n",
        "# Layer 5: \n",
        "This time around we have a convolutional layer with 5×5 kernel size and 120 filters. There is no need to even consider strides as the input size is 5x5x16 so we will get an output of 1x1x120. Total params in layer = 5 * 5 * 120 = 3000\n",
        "# Layer 6: \n",
        "This is a dense layer with 84 parameters. So, the input of 120 units is converted to 84 units. Total params = 84 * 120 + 84 = 10164. The activation function used here was rather a unique one. I’ll say you can just try out any of your choice here as the task is pretty simple one by today’s standards.\n",
        "# Output Layer: \n",
        "Finally, a dense layer with 10 units is used. Total params = 84 * 10 + 10 = 924.\n",
        "Skipping over the details of loss function used and why it was used, I would suggest using cross-entropy loss with softmax activation in the last layer. Try out different training schedules and learning rates.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8jJ8-iUKg3C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "855322e5-9dea-406b-9dd8-e45f361ed5b5"
      },
      "source": [
        "from keras import layers\n",
        "from keras.models import Model\n",
        "\n",
        "def lenet_5(in_shape=(32,32,1), n_classes=10, opt='sgd'):\n",
        "    in_layer = layers.Input(in_shape)\n",
        "    conv1 = layers.Conv2D(filters=20, kernel_size=5,\n",
        "                          padding='same', activation='relu')(in_layer)\n",
        "    pool1 = layers.MaxPool2D()(conv1)\n",
        "    conv2 = layers.Conv2D(filters=50, kernel_size=5,\n",
        "                          padding='same', activation='relu')(pool1)\n",
        "    pool2 = layers.MaxPool2D()(conv2)\n",
        "    flatten = layers.Flatten()(pool2)\n",
        "    dense1 = layers.Dense(500, activation='relu')(flatten)\n",
        "    preds = layers.Dense(n_classes, activation='softmax')(dense1)\n",
        "\n",
        "    model = Model(in_layer, preds)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\t              metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = lenet_5()\n",
        "    print(model.summary())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 32, 32, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 20)        520       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 50)        25050     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 50)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               1600500   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 1,631,080\n",
            "Trainable params: 1,631,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQOF7d6VLvKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}